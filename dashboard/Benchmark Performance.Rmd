---
title: "Benchmark Performance"
output: html_document
---

```{r setup, include=FALSE}

source("battle.r")
source("baseline.r")


# simulations_path <- "D:\\Documents\\autem\\benchmark\\simulations\\test\\Run_Next"
simulations_path <- "D:\\Documents\\autem\\benchmark\\simulations\\Run"
simulations_other_path <- "D:\\Documents\\autem\\benchmark\\simulations\\Run_2"
# simulations_other2_path <- "D:\\Documents\\autem\\benchmark\\simulations\\Run_3"
detail_version <- 4

baseline_path <- "D:\\Documents\\autem\\benchmark\\baselines"

knitr::opts_chunk$set(echo = FALSE)
```

# Load

```{r Load}

battle_df <- read_battle(simulations_path)
if (!is_null(simulations_other_path)) {
  battle_other_df <- read_battle(simulations_other_path)
  battle_df <- bind_rows(battle_df, battle_other_df)
}
#if (!is_null(simulations_other2_path)) {
#  battle_other_df <- read_battle(simulations_other2_path)
#  battle_df <- bind_rows(battle_df, battle_other_df)
#}
print("Battle data loaded")

```

# Simulation Summary

```{r}

error_df <- 
  battle_df %>%
  filter(fault != "None") %>%
  group_by(dataset, experiment, version, fault) %>%
  summarise(n = n()) %>%
  ungroup() %>%
  mutate(fault = str_sub(fault, 1, 80)) %>%
  select(dataset, experiment, version, n, fault)

error_count_df <-
  error_df %>%
  group_by(dataset, experiment, version) %>%
  summarise(n_error = sum(n)) %>%
  ungroup()
  

summary_df <-
  battle_df %>%
  filter(ranking == 1) %>%
  group_by(dataset, experiment, version) %>%
  left_join(error_count_df, by = c("dataset","experiment", "version")) %>%
  arrange(experiment, dataset, version) %>%
  select(experiment, dataset, version, rating, rating_sd, top_5p_accuracy, validation_accuracy, n_error) %>%
  ungroup()


print(summary_df, digits = 3)

```

# Data Set Performance

# Calculate performance

```{r}

baselines <- levels(factor(summary_df$dataset))

baseline_accuracy <- function(baseline_name) {
  
  baseline_df <- read_baseline_file(baseline_path, baseline_name)
  baseline_accuracy_df <- 
    baseline_df %>%
    mutate(
      experiment = "baseline",
      dataset = baseline_name,
      version = 0,
      accuracy = predictive_accuracy,
      accuracy_sd = NA,
      validation_accuracy = NA
    ) %>%
    select(experiment, dataset, version, accuracy, accuracy_sd, validation_accuracy)
  min_accuracy <- min(baseline_accuracy_df$accuracy)
  max_accuracy <- max(baseline_accuracy_df$accuracy)
  
  battle_accuracy_df <-
    summary_df %>%
    filter(dataset == baseline_name) %>%
    mutate(
      accuracy = rating,
      accuracy_sd = rating_sd
    ) %>%
    select(experiment, dataset, version, accuracy, accuracy_sd, validation_accuracy)

  accuracy_df <- 
    bind_rows(baseline_accuracy_df, battle_accuracy_df) %>%
    mutate(percentage = percent_rank(accuracy) * 100)
  
  accuracy_df$normal_accuracy <- (accuracy_df$accuracy - min_accuracy) / (max_accuracy - min_accuracy)
  accuracy_df$normal_accuracy_sd <- accuracy_df$accuracy_sd / (max_accuracy - min_accuracy)
  accuracy_df$normal_validation_accuracy <- (accuracy_df$validation_accuracy - min_accuracy) / (max_accuracy - min_accuracy)
  
  accuracy_df

}

baseline_accuracy_df <- baselines %>% map_dfr(baseline_accuracy)
baseline_accuracy_df %>% filter(!is.na(accuracy_sd))

```

## Plot combined performance

```{r}

plot(ggplot(data = baseline_accuracy_df, aes(x = percentage, y = normal_accuracy)) +
  geom_point(data = filter(baseline_accuracy_df, experiment == "baseline"), color = "Grey", size = .1) +
  geom_point(data = filter(baseline_accuracy_df, experiment == "Light"), aes(color = version), size = 2)
)

```

## Plot individual performance

```{r}
for (baseline_name in baselines) {
  
  baseline_df <- baseline_accuracy_df %>% filter(dataset == baseline_name, experiment == "baseline")
  experiment_df <- baseline_accuracy_df %>% filter(dataset == baseline_name, experiment != "baseline")

  plot(ggplot(experiment_df, aes(x = percentage, y = accuracy)) +
    geom_count(data = baseline_df, color = "DarkGrey") +
    geom_point(aes(shape = experiment, color = version), size = 3) +
    geom_rug(aes(color = version), sides="trbl") +
    geom_errorbar(aes(ymin=accuracy-accuracy_sd, ymax=accuracy+accuracy_sd), width = 2, na.rm = TRUE) +
    geom_point(aes(y = validation_accuracy, shape = "Validation", color = version), size = 3) +
    ggtitle(baseline_name)
  )

}


```



# Accuracy Contributors

## Learner

```{r}

for (baseline_name in baselines) {
  dataset_name <- baseline_name
  p <- battle_df %>%
    filter(dataset == dataset_name, experiment == "Light", !is.na(accuracy)) %>%
    ggplot(aes(x = step, y = accuracy)) +
    geom_point(aes(color = Learner), size = 0.5) +
    ggtitle(baseline_name) +
    facet_wrap(~ version)
  plot(p)
}


```

## Engineer

```{r}

for (baseline_name in baselines) {
  dataset_name <- baseline_name
  p <- battle_df %>%
    filter(dataset == dataset_name, experiment == "Light", !is.na(accuracy)) %>%
    ggplot(aes(x = step, y = accuracy)) +
    geom_point(aes(color = Engineer), size = 0.5) +
    ggtitle(baseline_name) +
    facet_wrap(~ version)
  plot(p)
}
```


## Scaler

```{r}

for (baseline_name in baselines) {
  dataset_name <- baseline_name
  p <- battle_df %>%
    filter(dataset == dataset_name, experiment == "Light", !is.na(accuracy)) %>%
    ggplot(aes(x = step, y = accuracy)) +
    geom_point(aes(color = Scaler), size = 0.5) +
    ggtitle(baseline_name) +
    facet_wrap(~ version)
  plot(p)
}
```

## Reducer

```{r}

for (baseline_name in baselines) {
  dataset_name <- baseline_name
  p <- battle_df %>%
    filter(dataset == dataset_name, experiment == "Light", !is.na(accuracy)) %>%
    ggplot(aes(x = step, y = accuracy)) +
    geom_point(aes(color = Reducer), size = 0.5) +
    ggtitle(baseline_name) +
    facet_wrap(~ version)
  plot(p)
}
```





