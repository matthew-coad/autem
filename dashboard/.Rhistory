ggtitle("Runtime versus Runtime per row")
runtime_summary_df %>%
filter(experiment == "Light") %>%
ggplot(aes(x = seconds, y = secs_per_value)) +
geom_text(aes(label = dataset), check_overlap = TRUE) +
xlab("Runtime (s)") +
ylab("Runtime per value") +
ggtitle("Runtime versus Runtime per value")
source("battle.r")
source("baseline.r")
source("configuration.r")
simulations_path1 <- "D:\\Documents\\autem\\benchmark\\simulations\\Run_5"
simulations_path2 <- NULL
simulations_path3 <- NULL
baseline_path <- "D:\\Documents\\autem\\benchmark\\baselines"
benchmark_path <- "D:\\Documents\\autem\\benchmark"
knitr::opts_chunk$set(echo = FALSE)
battle_df <- read_battle(simulations_path1)
if (!is_null(simulations_path2)) {
battle_other_df <- read_battle(simulations_path2)
battle_df <- bind_rows(battle_df, battle_other_df)
}
if (!is_null(simulations_path3)) {
battle_other_df <- read_battle(simulations_path3)
battle_df <- bind_rows(battle_df, battle_other_df)
}
battle_df <- clean_battle(battle_df)
configuration_df <- read_configuration_file(benchmark_path) %>% select(dataset = Name, everything())
print("Data loaded")
runtime_summary_df <-
battle_df %>%
group_by(experiment, dataset, version) %>%
summarise(
start_time = min(event_time),
end_time = max(event_time),
seconds = as.integer(lubridate::seconds(lubridate::interval(start_time, end_time))),
minutes = seconds / 60
) %>%
ungroup()
runtime_summary_df <-
runtime_summary_df %>%
inner_join(configuration_df, by = "dataset") %>%
select(
experiment, dataset, version, start_time, end_time, seconds, minutes,
classes = NumberOfClasses, features = NumberOfFeatures, instances = NumberOfInstances)
runtime_summary_df <-
runtime_summary_df %>%
mutate(
secs_per_instance = seconds / instances,
values = instances * features,
secs_per_value = seconds / values
)
runtime_summary_df %>%
arrange(desc(secs_per_value)) %>%
select(experiment, dataset, version, minutes, classes, features, instances, values, secs_per_value) %>% print()
runtime_summary_df %>%
filter(experiment == "Light") %>%
mutate(dataset = forcats::fct_reorder(dataset, seconds)) %>%
ggplot(aes(x = dataset, y = minutes)) +
ylab("Runtime (min)") +
facet_wrap(~ version) +
geom_col() +
coord_flip()
runtime_summary_df %>%
mutate(dataset = forcats::fct_reorder(dataset, secs_per_value)) %>%
ggplot(aes(x = dataset, y = secs_per_value)) +
ylab("Runtime (sec)") +
geom_col() +
coord_flip() +
facet_wrap(~ version) +
ggtitle("Runtime per value")
runtime_summary_df %>%
filter(experiment == "Light") %>%
ggplot(aes(x = seconds, y = secs_per_value)) +
geom_text(aes(label = dataset), check_overlap = TRUE) +
xlab("Runtime (s)") +
ylab("Runtime per value") +
ggtitle("Runtime versus Runtime per value")
runtime_summary_df %>%
ggplot(aes(x = seconds, y = secs_per_instance)) +
geom_text(aes(label = dataset), check_overlap = TRUE) +
xlab("Runtime (s)") +
ylab("Runtime per instance") +
ggtitle("Runtime versus Runtime per instance")
battle_df <- read_battle(simulations_path1)
if (!is_null(simulations_path2)) {
battle_other_df <- read_battle(simulations_path2)
battle_df <- bind_rows(battle_df, battle_other_df)
}
if (!is_null(simulations_path3)) {
battle_other_df <- read_battle(simulations_path3)
battle_df <- bind_rows(battle_df, battle_other_df)
}
print("Battle data loaded")
error_df <-
battle_df %>%
filter(fault != "None") %>%
group_by(dataset, experiment, version, fault) %>%
summarise(n = n()) %>%
ungroup() %>%
mutate(fault = str_sub(fault, 1, 80)) %>%
select(dataset, experiment, version, n, fault)
error_count_df <-
error_df %>%
group_by(dataset, experiment, version) %>%
summarise(n_error = sum(n)) %>%
ungroup()
summary_df <-
battle_df %>%
filter(ranking == 1) %>%
group_by(dataset, experiment, version) %>%
left_join(error_count_df, by = c("dataset","experiment", "version")) %>%
arrange(experiment, dataset, version) %>%
select(experiment, dataset, version, rating, rating_sd, top_5p_accuracy, validation_accuracy, n_error) %>%
ungroup()
baselines <- levels(factor(summary_df$dataset))
baseline_accuracy <- function(baseline_name) {
baseline_df <- read_baseline_file(baseline_path, baseline_name)
baseline_accuracy_df <-
baseline_df %>%
mutate(
experiment = "baseline",
dataset = baseline_name,
version = 0,
accuracy = predictive_accuracy,
accuracy_sd = NA,
validation_accuracy = NA
) %>%
select(experiment, dataset, version, accuracy, accuracy_sd, validation_accuracy)
min_accuracy <- min(baseline_accuracy_df$accuracy)
max_accuracy <- max(baseline_accuracy_df$accuracy)
battle_accuracy_df <-
summary_df %>%
filter(dataset == baseline_name) %>%
mutate(
accuracy = rating,
accuracy_sd = rating_sd
) %>%
select(experiment, dataset, version, accuracy, accuracy_sd, validation_accuracy)
accuracy_df <-
bind_rows(baseline_accuracy_df, battle_accuracy_df) %>%
mutate(percentage = percent_rank(accuracy) * 100)
accuracy_df$normal_accuracy <- (accuracy_df$accuracy - min_accuracy) / (max_accuracy - min_accuracy)
accuracy_df$normal_accuracy_sd <- accuracy_df$accuracy_sd / (max_accuracy - min_accuracy)
accuracy_df$normal_validation_accuracy <- (accuracy_df$validation_accuracy - min_accuracy) / (max_accuracy - min_accuracy)
accuracy_df
}
baseline_accuracy_df <- baselines %>% map_dfr(baseline_accuracy)
print(summary_df, digits = 3)
baseline_accuracy("vowel")
baseline_accuracy("analcatdata_dmft")
baseline_accuracy("analcatdata_dmft") %>% filter(experiment != "baseline")
source("battle.r")
source("baseline.r")
source("configuration.r")
simulations_path1 <- "D:\\Documents\\autem\\benchmark\\simulations\\Run_5"
simulations_path2 <- NULL
simulations_path3 <- NULL
baseline_path <- "D:\\Documents\\autem\\benchmark\\baselines"
benchmark_path <- "D:\\Documents\\autem\\benchmark"
knitr::opts_chunk$set(echo = FALSE)
battle_df <- read_battle(simulations_path1)
if (!is_null(simulations_path2)) {
battle_other_df <- read_battle(simulations_path2)
battle_df <- bind_rows(battle_df, battle_other_df)
}
if (!is_null(simulations_path3)) {
battle_other_df <- read_battle(simulations_path3)
battle_df <- bind_rows(battle_df, battle_other_df)
}
battle_df <- clean_battle(battle_df)
configuration_df <- read_configuration_file(benchmark_path) %>% select(dataset = Name, everything())
print("Data loaded")
runtime_summary_df <-
battle_df %>%
group_by(experiment, dataset, version) %>%
summarise(
start_time = min(event_time),
end_time = max(event_time),
seconds = as.integer(lubridate::seconds(lubridate::interval(start_time, end_time))),
minutes = seconds / 60
) %>%
ungroup()
runtime_summary_df <-
runtime_summary_df %>%
inner_join(configuration_df, by = "dataset") %>%
select(
experiment, dataset, version, start_time, end_time, seconds, minutes,
classes = NumberOfClasses, features = NumberOfFeatures, instances = NumberOfInstances)
runtime_summary_df <-
runtime_summary_df %>%
mutate(
secs_per_instance = seconds / instances,
values = instances * features,
secs_per_value = seconds / values
)
runtime_summary_df %>%
arrange(desc(secs_per_value)) %>%
select(experiment, dataset, version, minutes, classes, features, instances, values, secs_per_value) %>% print()
runtime_summary_df %>%
filter(experiment == "Light") %>%
mutate(dataset = forcats::fct_reorder(dataset, seconds)) %>%
ggplot(aes(x = dataset, y = minutes)) +
ylab("Runtime (min)") +
facet_wrap(~ version) +
geom_col() +
coord_flip()
runtime_summary_df %>%
mutate(dataset = forcats::fct_reorder(dataset, secs_per_value)) %>%
ggplot(aes(x = dataset, y = secs_per_value)) +
ylab("Runtime (sec)") +
geom_col() +
coord_flip() +
facet_wrap(~ version) +
ggtitle("Runtime per value")
runtime_summary_df %>%
filter(experiment == "Light") %>%
ggplot(aes(x = seconds, y = secs_per_value)) +
geom_text(aes(label = dataset), check_overlap = TRUE) +
xlab("Runtime (s)") +
ylab("Runtime per value") +
ggtitle("Runtime versus Runtime per value")
runtime_summary_df %>%
ggplot(aes(x = seconds, y = secs_per_instance)) +
geom_text(aes(label = dataset), check_overlap = TRUE) +
xlab("Runtime (s)") +
ylab("Runtime per instance") +
ggtitle("Runtime versus Runtime per instance")
source("battle.r")
source("baseline.r")
simulations_path1 <- "D:\\Documents\\autem\\benchmark\\simulations\\Run_5"
simulations_path2 <- NULL
simulations_path3 <- NULL
baseline_path <- "D:\\Documents\\autem\\benchmark\\baselines"
knitr::opts_chunk$set(echo = FALSE)
battle_df <- read_battle(simulations_path1)
if (!is_null(simulations_path2)) {
battle_other_df <- read_battle(simulations_path2)
battle_df <- bind_rows(battle_df, battle_other_df)
}
if (!is_null(simulations_path3)) {
battle_other_df <- read_battle(simulations_path3)
battle_df <- bind_rows(battle_df, battle_other_df)
}
print("Battle data loaded")
error_df <-
battle_df %>%
filter(fault != "None") %>%
group_by(dataset, experiment, version, fault) %>%
summarise(n = n()) %>%
ungroup() %>%
mutate(fault = str_sub(fault, 1, 80)) %>%
select(dataset, experiment, version, n, fault)
error_count_df <-
error_df %>%
group_by(dataset, experiment, version) %>%
summarise(n_error = sum(n)) %>%
ungroup()
summary_df <-
battle_df %>%
filter(ranking == 1) %>%
group_by(dataset, experiment, version) %>%
left_join(error_count_df, by = c("dataset","experiment", "version")) %>%
arrange(experiment, dataset, version) %>%
select(experiment, dataset, version, rating, rating_sd, top_5p_accuracy, validation_accuracy, n_error) %>%
ungroup()
baselines <- levels(factor(summary_df$dataset))
baseline_accuracy <- function(baseline_name) {
baseline_df <- read_baseline_file(baseline_path, baseline_name)
baseline_accuracy_df <-
baseline_df %>%
mutate(
experiment = "baseline",
dataset = baseline_name,
version = 0,
accuracy = predictive_accuracy,
accuracy_sd = NA,
validation_accuracy = NA
) %>%
select(experiment, dataset, version, accuracy, accuracy_sd, validation_accuracy)
min_accuracy <- min(baseline_accuracy_df$accuracy)
max_accuracy <- max(baseline_accuracy_df$accuracy)
battle_accuracy_df <-
summary_df %>%
filter(dataset == baseline_name) %>%
mutate(
accuracy = rating,
accuracy_sd = rating_sd
) %>%
select(experiment, dataset, version, accuracy, accuracy_sd, validation_accuracy)
accuracy_df <-
bind_rows(baseline_accuracy_df, battle_accuracy_df) %>%
mutate(percentage = percent_rank(accuracy) * 100)
accuracy_df$normal_accuracy <- (accuracy_df$accuracy - min_accuracy) / (max_accuracy - min_accuracy)
accuracy_df$normal_accuracy_sd <- accuracy_df$accuracy_sd / (max_accuracy - min_accuracy)
accuracy_df$normal_validation_accuracy <- (accuracy_df$validation_accuracy - min_accuracy) / (max_accuracy - min_accuracy)
accuracy_df
}
baseline_accuracy_df <- baselines %>% map_dfr(baseline_accuracy)
print(summary_df, digits = 3)
baseline_accuracy("analcatdata_dmft") %>% filter(experiment != "baseline")
plot(ggplot(data = baseline_accuracy_df, aes(x = percentage, y = normal_accuracy)) +
geom_point(data = filter(baseline_accuracy_df, experiment == "baseline"), color = "Grey", size = .1) +
geom_point(data = filter(baseline_accuracy_df, experiment == "Light"), aes(color = version), size = 2)
)
for (baseline_name in baselines) {
baseline_df <- baseline_accuracy_df %>% filter(dataset == baseline_name, experiment == "baseline")
experiment_df <- baseline_accuracy_df %>% filter(dataset == baseline_name, experiment != "baseline")
plot(ggplot(experiment_df, aes(x = percentage, y = accuracy)) +
geom_count(data = baseline_df, color = "DarkGrey") +
geom_point(aes(shape = experiment, color = version), size = 3) +
geom_rug(aes(color = version), sides="trbl") +
geom_errorbar(aes(ymin=accuracy-accuracy_sd, ymax=accuracy+accuracy_sd), width = 2, na.rm = TRUE) +
geom_point(aes(y = validation_accuracy, shape = "Validation", color = version), size = 3) +
ggtitle(baseline_name)
)
}
for (baseline_name in baselines) {
dataset_name <- baseline_name
p <- battle_df %>%
filter(dataset == dataset_name, experiment == "Light", !is.na(accuracy)) %>%
ggplot(aes(x = step, y = accuracy)) +
geom_point(aes(color = Learner), size = 0.5) +
ggtitle(baseline_name) +
facet_wrap(~ version)
plot(p)
}
for (baseline_name in baselines) {
dataset_name <- baseline_name
p <- battle_df %>%
filter(dataset == dataset_name, experiment == "Light", !is.na(accuracy)) %>%
ggplot(aes(x = step, y = accuracy)) +
geom_point(aes(color = Scaler), size = 0.5) +
ggtitle(baseline_name) +
facet_wrap(~ version)
plot(p)
}
source("battle.r")
source("baseline.r")
simulations_path1 <- "D:\\Documents\\autem\\benchmark\\simulations\\Run_5"
simulations_path2 <- NULL
simulations_path3 <- NULL
baseline_path <- "D:\\Documents\\autem\\benchmark\\baselines"
knitr::opts_chunk$set(echo = FALSE)
battle_df <- read_battle(simulations_path1)
if (!is_null(simulations_path2)) {
battle_other_df <- read_battle(simulations_path2)
battle_df <- bind_rows(battle_df, battle_other_df)
}
if (!is_null(simulations_path3)) {
battle_other_df <- read_battle(simulations_path3)
battle_df <- bind_rows(battle_df, battle_other_df)
}
print("Battle data loaded")
error_df <-
battle_df %>%
filter(fault != "None") %>%
group_by(dataset, experiment, version, fault) %>%
summarise(n = n()) %>%
ungroup() %>%
mutate(fault = str_sub(fault, 1, 80)) %>%
select(dataset, experiment, version, n, fault)
error_count_df <-
error_df %>%
group_by(dataset, experiment, version) %>%
summarise(n_error = sum(n)) %>%
ungroup()
summary_df <-
battle_df %>%
filter(ranking == 1) %>%
group_by(dataset, experiment, version) %>%
left_join(error_count_df, by = c("dataset","experiment", "version")) %>%
arrange(experiment, dataset, version) %>%
select(experiment, dataset, version, rating, rating_sd, top_5p_accuracy, validation_accuracy, n_error) %>%
ungroup()
baselines <- levels(factor(summary_df$dataset))
baseline_accuracy <- function(baseline_name) {
baseline_df <- read_baseline_file(baseline_path, baseline_name) %>%
filter(!is.na(predictive_accuracy))
baseline_accuracy_df <-
baseline_df %>%
mutate(
experiment = "baseline",
dataset = baseline_name,
version = 0,
accuracy = predictive_accuracy,
accuracy_sd = NA,
validation_accuracy = NA
) %>%
select(experiment, dataset, version, accuracy, accuracy_sd, validation_accuracy)
min_accuracy <- min(baseline_accuracy_df$accuracy)
max_accuracy <- max(baseline_accuracy_df$accuracy)
battle_accuracy_df <-
summary_df %>%
filter(dataset == baseline_name) %>%
mutate(
accuracy = rating,
accuracy_sd = rating_sd
) %>%
select(experiment, dataset, version, accuracy, accuracy_sd, validation_accuracy)
accuracy_df <-
bind_rows(baseline_accuracy_df, battle_accuracy_df) %>%
mutate(percentage = percent_rank(accuracy) * 100)
accuracy_df$normal_accuracy <- (accuracy_df$accuracy - min_accuracy) / (max_accuracy - min_accuracy)
accuracy_df$normal_accuracy_sd <- accuracy_df$accuracy_sd / (max_accuracy - min_accuracy)
accuracy_df$normal_validation_accuracy <- (accuracy_df$validation_accuracy - min_accuracy) / (max_accuracy - min_accuracy)
accuracy_df
}
baseline_accuracy_df <- baselines %>% map_dfr(baseline_accuracy)
print(summary_df, digits = 3)
plot(ggplot(data = baseline_accuracy_df, aes(x = percentage, y = normal_accuracy)) +
geom_point(data = filter(baseline_accuracy_df, experiment == "baseline"), color = "Grey", size = .1) +
geom_point(data = filter(baseline_accuracy_df, experiment == "Light"), aes(color = version), size = 2)
)
for (baseline_name in baselines) {
baseline_df <- baseline_accuracy_df %>% filter(dataset == baseline_name, experiment == "baseline")
experiment_df <- baseline_accuracy_df %>% filter(dataset == baseline_name, experiment != "baseline")
plot(ggplot(experiment_df, aes(x = percentage, y = accuracy)) +
geom_count(data = baseline_df, color = "DarkGrey") +
geom_point(aes(shape = experiment, color = version), size = 3) +
geom_rug(aes(color = version), sides="trbl") +
geom_errorbar(aes(ymin=accuracy-accuracy_sd, ymax=accuracy+accuracy_sd), width = 2, na.rm = TRUE) +
geom_point(aes(y = validation_accuracy, shape = "Validation", color = version), size = 3) +
ggtitle(baseline_name)
)
}
for (baseline_name in baselines) {
dataset_name <- baseline_name
p <- battle_df %>%
filter(dataset == dataset_name, experiment == "Light", !is.na(accuracy)) %>%
ggplot(aes(x = step, y = accuracy)) +
geom_point(aes(color = Scaler), size = 0.5) +
ggtitle(baseline_name) +
facet_wrap(~ version)
plot(p)
}
source("battle.r")
source("baseline.r")
source("configuration.r")
simulations_path1 <- "D:\\Documents\\autem\\benchmark\\simulations\\Run_5"
simulations_path2 <- NULL
simulations_path3 <- NULL
baseline_path <- "D:\\Documents\\autem\\benchmark\\baselines"
benchmark_path <- "D:\\Documents\\autem\\benchmark"
knitr::opts_chunk$set(echo = FALSE)
battle_df <- read_battle(simulations_path1)
if (!is_null(simulations_path2)) {
battle_other_df <- read_battle(simulations_path2)
battle_df <- bind_rows(battle_df, battle_other_df)
}
if (!is_null(simulations_path3)) {
battle_other_df <- read_battle(simulations_path3)
battle_df <- bind_rows(battle_df, battle_other_df)
}
battle_df <- clean_battle(battle_df)
configuration_df <- read_configuration_file(benchmark_path) %>% select(dataset = Name, everything())
print("Data loaded")
runtime_summary_df <-
battle_df %>%
group_by(experiment, dataset, version) %>%
summarise(
start_time = min(event_time),
end_time = max(event_time),
seconds = as.integer(lubridate::seconds(lubridate::interval(start_time, end_time))),
minutes = seconds / 60
) %>%
ungroup()
runtime_summary_df <-
runtime_summary_df %>%
inner_join(configuration_df, by = "dataset") %>%
select(
experiment, dataset, version, start_time, end_time, seconds, minutes,
classes = NumberOfClasses, features = NumberOfFeatures, instances = NumberOfInstances)
runtime_summary_df <-
runtime_summary_df %>%
mutate(
secs_per_instance = seconds / instances,
values = instances * features,
secs_per_value = seconds / values
)
runtime_summary_df %>%
arrange(desc(secs_per_value)) %>%
select(experiment, dataset, version, minutes, classes, features, instances, values, secs_per_value) %>% print()
runtime_summary_df %>%
filter(experiment == "Light") %>%
mutate(dataset = forcats::fct_reorder(dataset, seconds)) %>%
ggplot(aes(x = dataset, y = minutes)) +
ylab("Runtime (min)") +
facet_wrap(~ version) +
geom_col() +
coord_flip()
runtime_summary_df %>%
mutate(dataset = forcats::fct_reorder(dataset, secs_per_value)) %>%
ggplot(aes(x = dataset, y = secs_per_value)) +
ylab("Runtime (sec)") +
geom_col() +
coord_flip() +
facet_wrap(~ version) +
ggtitle("Runtime per value")
runtime_summary_df %>%
filter(experiment == "Light") %>%
ggplot(aes(x = seconds, y = secs_per_value)) +
geom_text(aes(label = dataset), check_overlap = TRUE) +
xlab("Runtime (s)") +
ylab("Runtime per value") +
ggtitle("Runtime versus Runtime per value")
runtime_summary_df %>%
ggplot(aes(x = seconds, y = secs_per_instance)) +
geom_text(aes(label = dataset), check_overlap = TRUE) +
xlab("Runtime (s)") +
ylab("Runtime per instance") +
ggtitle("Runtime versus Runtime per instance")
