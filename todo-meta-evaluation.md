# Meta Evaluation

The theme is this section is to be able to evaluate the performance of algorithms across a wide variety of problems.

## Tasks

Only generate a ranking when the underlying forms have changed. **done**

Add cross validation score to rankings.  **done**

View cross validation score on dashboard.  **done**

Add meta information about experiment to simulations. **done**

Load experiment data into a single data frame with meta information. **done**

Add multiple experiment views. **done**

Add learner tuning **done**

Survival/attractiveness rated on recent contests **done**

Ranking in battle reports **done**

Add simulation event field **done**

Download openml runs related to a run **manual task**
